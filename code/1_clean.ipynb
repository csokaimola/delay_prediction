{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d219ac",
   "metadata": {},
   "source": [
    "This notebook does:\n",
    "* Inspect the 5 dataset.\n",
    "* Concatenate, clean data and feature engineering.\n",
    "* Create **data quality reports with sweetviz**.\n",
    "* Short summary of **findings about descriptive statistics**.\n",
    "* Add a variable \"domestic\" from external data (then drop because all were domestic).\n",
    "* Create the **data/cleaned_df.parquet**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe171a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All files have the same variables (columns).\n"
     ]
    }
   ],
   "source": [
    "# Check whether all csv files contain the same variables.\n",
    "import pandas as pd\n",
    "\n",
    "# List of your CSV files (with folder path)\n",
    "files = [f\"data/raw/{year}_.csv\" for year in range(2014, 2019)]\n",
    "\n",
    "# Dictionary to hold column sets\n",
    "columns_dict = {}\n",
    "\n",
    "# Read each file and store its columns\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, nrows=0)  # only read header\n",
    "    columns_dict[file] = set(df.columns)\n",
    "\n",
    "# Compare columns across files\n",
    "all_equal = all(cols == list(columns_dict.values())[0] for cols in columns_dict.values())\n",
    "\n",
    "if all_equal:\n",
    "    print(\"✅ All files have the same variables (columns).\")\n",
    "else:\n",
    "    print(\"❌ Files have different variables.\")\n",
    "    for file, cols in columns_dict.items():\n",
    "        print(f\"\\n{file}:\")\n",
    "        print(sorted(cols))\n",
    "\n",
    "    # Show differences between files\n",
    "    common = set.intersection(*columns_dict.values())\n",
    "    union = set.union(*columns_dict.values())\n",
    "    print(\"\\nVariables missing in some files:\")\n",
    "    print(sorted(union - common))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f159780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csoka\\AppData\\Local\\Temp\\ipykernel_1936\\350552192.py:10: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>OP_CARRIER</th>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>...</th>\n",
       "      <th>ACTUAL_ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>Unnamed..27</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11484</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>AS</td>\n",
       "      <td>1</td>\n",
       "      <td>DCA</td>\n",
       "      <td>SEA</td>\n",
       "      <td>745</td>\n",
       "      <td>733.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>349.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>2329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11485</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>AS</td>\n",
       "      <td>2</td>\n",
       "      <td>SEA</td>\n",
       "      <td>DCA</td>\n",
       "      <td>1410</td>\n",
       "      <td>1405.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>278.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>2329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11486</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>AS</td>\n",
       "      <td>3</td>\n",
       "      <td>DCA</td>\n",
       "      <td>SEA</td>\n",
       "      <td>1840</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>339.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>2329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11487</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>AS</td>\n",
       "      <td>4</td>\n",
       "      <td>SEA</td>\n",
       "      <td>DCA</td>\n",
       "      <td>810</td>\n",
       "      <td>810.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>294.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>2329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11488</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>AS</td>\n",
       "      <td>5</td>\n",
       "      <td>EWR</td>\n",
       "      <td>SEA</td>\n",
       "      <td>1825</td>\n",
       "      <td>1829.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>371.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>2402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     FL_DATE OP_CARRIER  OP_CARRIER_FL_NUM ORIGIN DEST  \\\n",
       "0       11484  2014-01-01         AS                  1    DCA  SEA   \n",
       "1       11485  2014-01-01         AS                  2    SEA  DCA   \n",
       "2       11486  2014-01-01         AS                  3    DCA  SEA   \n",
       "3       11487  2014-01-01         AS                  4    SEA  DCA   \n",
       "4       11488  2014-01-01         AS                  5    EWR  SEA   \n",
       "\n",
       "   CRS_DEP_TIME  DEP_TIME  DEP_DELAY  TAXI_OUT  ...  ACTUAL_ELAPSED_TIME  \\\n",
       "0           745     733.0      -12.0       8.0  ...                349.0   \n",
       "1          1410    1405.0       -5.0      13.0  ...                278.0   \n",
       "2          1840    1839.0       -1.0      15.0  ...                339.0   \n",
       "3           810     810.0        0.0      31.0  ...                294.0   \n",
       "4          1825    1829.0        4.0      20.0  ...                371.0   \n",
       "\n",
       "   AIR_TIME  DISTANCE  CARRIER_DELAY  WEATHER_DELAY  NAS_DELAY  \\\n",
       "0     332.0      2329            NaN            NaN        NaN   \n",
       "1     261.0      2329            NaN            NaN        NaN   \n",
       "2     320.0      2329            NaN            NaN        NaN   \n",
       "3     259.0      2329            NaN            NaN        NaN   \n",
       "4     347.0      2402            NaN            NaN        NaN   \n",
       "\n",
       "   SECURITY_DELAY LATE_AIRCRAFT_DELAY  Unnamed..27  year  \n",
       "0             NaN                 NaN          NaN  2014  \n",
       "1             NaN                 NaN          NaN  2014  \n",
       "2             NaN                 NaN          NaN  2014  \n",
       "3             NaN                 NaN          NaN  2014  \n",
       "4             NaN                 NaN          NaN  2014  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data: append the data files and show df\n",
    "\n",
    "# Build list of file paths\n",
    "files = [f\"data/raw/{year}_.csv\" for year in range(2014, 2019)]\n",
    "\n",
    "# Load all files into a list of DataFrames with a year column\n",
    "dfs = []\n",
    "for file in files:\n",
    "    year = os.path.basename(file).split(\"_\")[0]   # extract '2014' from '2014_.csv'\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"year\"] = int(year)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all into one DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Show the first rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e5d723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether all op_carrier values are AS. If yes, then it contains no information, we can drop it\n",
    "# True if every row has OP_CARRIER == \"AS\"\n",
    "df[\"OP_CARRIER\"].eq(\"AS\").all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "981fdd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1455.000000\n",
      "mean        0.845410\n",
      "std         0.207236\n",
      "min         0.183071\n",
      "25%         0.700000\n",
      "50%         0.976641\n",
      "75%         1.000000\n",
      "max         1.000000\n",
      "Name: consistency, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check whether op_carrier_fl_num contains extra information (other than route: destination and origin)\n",
    "df['flight_id'] = df['OP_CARRIER'].astype(str) + '_' + df['OP_CARRIER_FL_NUM'].astype(str)\n",
    "df['route'] = df['ORIGIN'] + '-' + df['DEST']\n",
    "\n",
    "cons = (df.groupby('flight_id')['route']\n",
    "          .agg(n='size', n_mode=lambda s: s.value_counts().iloc[0],\n",
    "               route_mode=lambda s: s.value_counts().idxmax()))\n",
    "cons['consistency'] = cons['n_mode'] / cons['n']\n",
    "print(cons['consistency'].describe())   # Calculates, per flight_id, the share of records on its dominant route.\n",
    "# if ~>0.9, then flight_id tipically means the same route (in 90% it flies the same route, in 10% not)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bde523",
   "metadata": {},
   "source": [
    "It suggests that op_carrier_fl_num contains limited extra signal (on top of destination and origin). So I drop it, but might worth reconsidering. For generalizable model one should drop it, for predicting within a small time frame if could be useful (might contain extra info on rotation, air craft type, other operational characteristics eg. gate, programs such as shuttle/seasonal/long/short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82fa3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0               int64\n",
       "FL_DATE                 object\n",
       "OP_CARRIER              object\n",
       "OP_CARRIER_FL_NUM        int64\n",
       "ORIGIN                  object\n",
       "DEST                    object\n",
       "CRS_DEP_TIME             int64\n",
       "DEP_TIME               float64\n",
       "DEP_DELAY              float64\n",
       "TAXI_OUT               float64\n",
       "WHEELS_OFF             float64\n",
       "WHEELS_ON              float64\n",
       "TAXI_IN                float64\n",
       "CRS_ARR_TIME             int64\n",
       "ARR_TIME               float64\n",
       "ARR_DELAY              float64\n",
       "CANCELLED              float64\n",
       "CANCELLATION_CODE       object\n",
       "DIVERTED               float64\n",
       "CRS_ELAPSED_TIME         int64\n",
       "ACTUAL_ELAPSED_TIME    float64\n",
       "AIR_TIME               float64\n",
       "DISTANCE                 int64\n",
       "CARRIER_DELAY          float64\n",
       "WEATHER_DELAY          float64\n",
       "NAS_DELAY              float64\n",
       "SECURITY_DELAY         float64\n",
       "LATE_AIRCRAFT_DELAY    float64\n",
       "Unnamed..27            float64\n",
       "year                     int64\n",
       "flight_id               object\n",
       "route                   object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check variable types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cb14fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fl_date</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>crs_dep_time</th>\n",
       "      <th>dep_delay</th>\n",
       "      <th>crs_arr_time</th>\n",
       "      <th>crs_elapsed_time</th>\n",
       "      <th>distance</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>DCA</td>\n",
       "      <td>SEA</td>\n",
       "      <td>07:45:00</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>10:37:00</td>\n",
       "      <td>352</td>\n",
       "      <td>2329</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>SEA</td>\n",
       "      <td>DCA</td>\n",
       "      <td>14:10:00</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>21:56:00</td>\n",
       "      <td>286</td>\n",
       "      <td>2329</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>DCA</td>\n",
       "      <td>SEA</td>\n",
       "      <td>18:40:00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>21:45:00</td>\n",
       "      <td>365</td>\n",
       "      <td>2329</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>SEA</td>\n",
       "      <td>DCA</td>\n",
       "      <td>08:10:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16:06:00</td>\n",
       "      <td>296</td>\n",
       "      <td>2329</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>EWR</td>\n",
       "      <td>SEA</td>\n",
       "      <td>18:25:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21:52:00</td>\n",
       "      <td>387</td>\n",
       "      <td>2402</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fl_date origin dest crs_dep_time  dep_delay crs_arr_time  \\\n",
       "0  2014-01-01    DCA  SEA     07:45:00      -12.0     10:37:00   \n",
       "1  2014-01-01    SEA  DCA     14:10:00       -5.0     21:56:00   \n",
       "2  2014-01-01    DCA  SEA     18:40:00       -1.0     21:45:00   \n",
       "3  2014-01-01    SEA  DCA     08:10:00        0.0     16:06:00   \n",
       "4  2014-01-01    EWR  SEA     18:25:00        4.0     21:52:00   \n",
       "\n",
       "   crs_elapsed_time  distance  year  \n",
       "0               352      2329  2014  \n",
       "1               286      2329  2014  \n",
       "2               365      2329  2014  \n",
       "3               296      2329  2014  \n",
       "4               387      2402  2014  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only those that can be used for prediction, rename to snake_case and convert to proper varable type\n",
    "# 1) Keep only the requested columns\n",
    "keep = [\n",
    "    \"FL_DATE\", \"ORIGIN\", \"DEST\",\n",
    "    \"CRS_DEP_TIME\", \"DEP_DELAY\", \"CRS_ARR_TIME\", \"CRS_ELAPSED_TIME\",\n",
    "    \"DISTANCE\", \"year\"\n",
    "]\n",
    "\n",
    "df = df[keep].copy()\n",
    "\n",
    "# 2) Rename to snake_case\n",
    "df = df.rename(columns={\n",
    "    \"FL_DATE\": \"fl_date\",\n",
    "    \"ORIGIN\": \"origin\",\n",
    "    \"DEST\": \"dest\",\n",
    "    \"CRS_DEP_TIME\": \"crs_dep_time\",\n",
    "    \"DEP_DELAY\": \"dep_delay\",\n",
    "    \"CRS_ARR_TIME\": \"crs_arr_time\",\n",
    "    \"CRS_ELAPSED_TIME\": \"crs_elapsed_time\",\n",
    "    \"DISTANCE\": \"distance\",\n",
    "})\n",
    "\n",
    "# --- helpers ---\n",
    "def hhmm_to_time(series):\n",
    "    \"\"\"\n",
    "    Convert hhmm integers (e.g., 745 -> 07:45) to datetime.time.\n",
    "    Handles floats/strings/NaN safely. Treats 2400 as 0000.\n",
    "    \"\"\"\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").round().astype(\"Int64\")\n",
    "    s = s.where(s != 2400, 0)  # 2400 -> 0000\n",
    "    t = pd.to_datetime(s.astype(str).str.zfill(4), format=\"%H%M\", errors=\"coerce\")\n",
    "    return t.dt.time  # object dtype of datetime.time\n",
    "\n",
    "# 3) Type conversions\n",
    "# fl_date → date\n",
    "df[\"fl_date\"] = pd.to_datetime(df[\"fl_date\"], errors=\"coerce\").dt.date\n",
    "\n",
    "# origin, dest → category\n",
    "df[\"origin\"] = df[\"origin\"].astype(\"category\")\n",
    "df[\"dest\"]   = df[\"dest\"].astype(\"category\")\n",
    "\n",
    "# CRS times → time/duration\n",
    "df[\"crs_dep_time\"] = hhmm_to_time(df[\"crs_dep_time\"])\n",
    "df[\"crs_arr_time\"] = hhmm_to_time(df[\"crs_arr_time\"])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce6e6136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fl_date               object\n",
       "origin              category\n",
       "dest                category\n",
       "crs_dep_time          object\n",
       "dep_delay            float64\n",
       "crs_arr_time          object\n",
       "crs_elapsed_time       int64\n",
       "distance               int64\n",
       "year                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01a006",
   "metadata": {},
   "source": [
    "Feature engineering: \n",
    "1. route (from origin and dest) \n",
    "2. proxy for holidays, peak season\n",
    "3. weekday, weekend\n",
    "4. hour, month, season\n",
    "5. distance bucket\n",
    "6. bank pressure (origin departures per hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88fcbc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\csoka\\AppData\\Local\\Temp\\ipykernel_1936\\507700364.py:90: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df.groupby([\"origin\", \"_dep_date\", \"_dep_hour\"])[\"origin\"]\n"
     ]
    }
   ],
   "source": [
    "# Safe datetime view of fl_date (doesn't overwrite your column)\n",
    "dt = pd.to_datetime(df[\"fl_date\"], errors=\"coerce\")\n",
    "\n",
    "def extract_hour_from_time(col):\n",
    "    \"\"\"Gets hour from either timedelta64 or Python datetime.time (object).\"\"\"\n",
    "    if pd.api.types.is_timedelta64_dtype(df[col]):\n",
    "        return df[col].dt.components.hours\n",
    "    return df[col].apply(lambda t: t.hour if pd.notnull(t) else np.nan)\n",
    "\n",
    "# ----- requested features -----\n",
    "\n",
    "# 1) Route\n",
    "df[\"route\"] = df[\"origin\"].astype(str) + \"-\" + df[\"dest\"].astype(str)\n",
    "\n",
    "# 2) Holiday / peak season proxies (US-centric)\n",
    "cal = USFederalHolidayCalendar()\n",
    "if dt.notna().any():\n",
    "    holidays = cal.holidays(start=dt.min(), end=dt.max())\n",
    "    df[\"is_us_holiday\"] = dt.isin(holidays).fillna(False)\n",
    "\n",
    "    if len(holidays):\n",
    "        near = pd.DatetimeIndex(holidays).union(holidays - pd.Timedelta(days=1)).union(holidays + pd.Timedelta(days=1))\n",
    "        df[\"near_holiday\"] = dt.isin(near).fillna(False)\n",
    "    else:\n",
    "        df[\"near_holiday\"] = False\n",
    "else:\n",
    "    df[\"is_us_holiday\"] = False\n",
    "    df[\"near_holiday\"] = False\n",
    "\n",
    "df[\"is_summer_peak\"] = dt.dt.month.isin([6, 7, 8]).fillna(False)\n",
    "\n",
    "def thanksgiving_date(y):\n",
    "    first = pd.Timestamp(y, 11, 1)\n",
    "    first_thu = first + pd.Timedelta(days=(3 - first.weekday()) % 7)\n",
    "    return first_thu + pd.Timedelta(days=21)\n",
    "\n",
    "is_tg = pd.Series(False, index=df.index)\n",
    "for y in pd.Index(dt.dt.year.dropna().astype(int).unique()):\n",
    "    tg = thanksgiving_date(int(y))\n",
    "    is_tg |= (dt >= tg - pd.Timedelta(days=3)) & (dt <= tg + pd.Timedelta(days=3))\n",
    "df[\"is_thanksgiving_week\"] = is_tg\n",
    "\n",
    "df[\"is_xmas_newyear\"] = ((dt.dt.month == 12) & (dt.dt.day >= 20)) | ((dt.dt.month == 1) & (dt.dt.day <= 6))\n",
    "df[\"is_xmas_newyear\"] = df[\"is_xmas_newyear\"].fillna(False)\n",
    "\n",
    "df[\"is_peak_season\"] = df[[\"is_summer_peak\", \"is_thanksgiving_week\", \"is_xmas_newyear\", \"near_holiday\"]].any(axis=1)\n",
    "\n",
    "# 3) Weekday / Weekend\n",
    "df[\"weekday\"] = dt.dt.day_name()\n",
    "df[\"is_weekend\"] = (dt.dt.weekday >= 5).fillna(False)\n",
    "\n",
    "# 4) Time-related\n",
    "\n",
    "# Departure hour (0–23) from scheduled dep time\n",
    "df[\"dep_hour\"] = extract_hour_from_time(\"crs_dep_time\")\n",
    "\n",
    "# Month / season / year\n",
    "df[\"month\"] = dt.dt.month\n",
    "df[\"season\"] = pd.Categorical(\n",
    "    np.select(\n",
    "        [\n",
    "            df[\"month\"].isin([12, 1, 2]),\n",
    "            df[\"month\"].isin([3, 4, 5]),\n",
    "            df[\"month\"].isin([6, 7, 8]),\n",
    "            df[\"month\"].isin([9, 10, 11]),\n",
    "        ],\n",
    "        [\"winter\", \"spring\", \"summer\", \"autumn\"],\n",
    "        default=\"unknown\",\n",
    "    ),\n",
    "    ordered=True, categories=[\"winter\", \"spring\", \"summer\", \"autumn\"]\n",
    ")\n",
    "\n",
    "# Convert to categorical\n",
    "df[\"year\"] = df[\"year\"].astype(\"category\")\n",
    "df[\"dep_hour\"] = df[\"dep_hour\"].astype(\"category\")\n",
    "df[\"month\"] = df[\"month\"].astype(\"category\")\n",
    "\n",
    "# 5) Distance buckets\n",
    "df[\"distance_bin\"] = pd.cut(pd.to_numeric(df[\"distance\"], errors=\"coerce\"),\n",
    "                            bins=[-np.inf, 300, 1000, 2000, np.inf],\n",
    "                            labels=[\"short\", \"medium\", \"long\", \"ultra\"],\n",
    "                            ordered=True)\n",
    "\n",
    "# 6) Bank pressure (origin departures per hour)\n",
    "# Count scheduled departures per (origin, date, dep_hour)\n",
    "df[\"_dep_date\"] = dt.dt.date\n",
    "df[\"_dep_hour\"] = df[\"dep_hour\"].astype(\"Int64\")   # may be NA if time missing\n",
    "\n",
    "df[\"bank_pressure_hour\"] = (\n",
    "    df.groupby([\"origin\", \"_dep_date\", \"_dep_hour\"])[\"origin\"]\n",
    "      .transform(\"size\")\n",
    "      .astype(\"Int64\")\n",
    ")\n",
    "\n",
    "# optional: fill missing with 0 if you prefer numeric\n",
    "# df[\"bank_pressure_hour\"] = df[\"bank_pressure_hour\"].fillna(0).astype(\"Int64\")\n",
    "\n",
    "# clean up helper columns\n",
    "df.drop(columns=[\"_dep_date\", \"_dep_hour\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2264f551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fl_date</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>crs_dep_time</th>\n",
       "      <th>dep_delay</th>\n",
       "      <th>crs_arr_time</th>\n",
       "      <th>crs_elapsed_time</th>\n",
       "      <th>distance</th>\n",
       "      <th>year</th>\n",
       "      <th>route</th>\n",
       "      <th>...</th>\n",
       "      <th>is_thanksgiving_week</th>\n",
       "      <th>is_xmas_newyear</th>\n",
       "      <th>is_peak_season</th>\n",
       "      <th>weekday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>dep_hour</th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "      <th>distance_bin</th>\n",
       "      <th>bank_pressure_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>DCA</td>\n",
       "      <td>SEA</td>\n",
       "      <td>07:45:00</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>10:37:00</td>\n",
       "      <td>352</td>\n",
       "      <td>2329</td>\n",
       "      <td>2014</td>\n",
       "      <td>DCA-SEA</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>winter</td>\n",
       "      <td>ultra</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>SEA</td>\n",
       "      <td>DCA</td>\n",
       "      <td>14:10:00</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>21:56:00</td>\n",
       "      <td>286</td>\n",
       "      <td>2329</td>\n",
       "      <td>2014</td>\n",
       "      <td>SEA-DCA</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>winter</td>\n",
       "      <td>ultra</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>DCA</td>\n",
       "      <td>SEA</td>\n",
       "      <td>18:40:00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>21:45:00</td>\n",
       "      <td>365</td>\n",
       "      <td>2329</td>\n",
       "      <td>2014</td>\n",
       "      <td>DCA-SEA</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>winter</td>\n",
       "      <td>ultra</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>SEA</td>\n",
       "      <td>DCA</td>\n",
       "      <td>08:10:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16:06:00</td>\n",
       "      <td>296</td>\n",
       "      <td>2329</td>\n",
       "      <td>2014</td>\n",
       "      <td>SEA-DCA</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>winter</td>\n",
       "      <td>ultra</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>EWR</td>\n",
       "      <td>SEA</td>\n",
       "      <td>18:25:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21:52:00</td>\n",
       "      <td>387</td>\n",
       "      <td>2402</td>\n",
       "      <td>2014</td>\n",
       "      <td>EWR-SEA</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>winter</td>\n",
       "      <td>ultra</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fl_date origin dest crs_dep_time  dep_delay crs_arr_time  \\\n",
       "0  2014-01-01    DCA  SEA     07:45:00      -12.0     10:37:00   \n",
       "1  2014-01-01    SEA  DCA     14:10:00       -5.0     21:56:00   \n",
       "2  2014-01-01    DCA  SEA     18:40:00       -1.0     21:45:00   \n",
       "3  2014-01-01    SEA  DCA     08:10:00        0.0     16:06:00   \n",
       "4  2014-01-01    EWR  SEA     18:25:00        4.0     21:52:00   \n",
       "\n",
       "   crs_elapsed_time  distance  year    route  ...  is_thanksgiving_week  \\\n",
       "0               352      2329  2014  DCA-SEA  ...                 False   \n",
       "1               286      2329  2014  SEA-DCA  ...                 False   \n",
       "2               365      2329  2014  DCA-SEA  ...                 False   \n",
       "3               296      2329  2014  SEA-DCA  ...                 False   \n",
       "4               387      2402  2014  EWR-SEA  ...                 False   \n",
       "\n",
       "   is_xmas_newyear  is_peak_season    weekday  is_weekend  dep_hour month  \\\n",
       "0             True            True  Wednesday       False         7     1   \n",
       "1             True            True  Wednesday       False        14     1   \n",
       "2             True            True  Wednesday       False        18     1   \n",
       "3             True            True  Wednesday       False         8     1   \n",
       "4             True            True  Wednesday       False        18     1   \n",
       "\n",
       "   season distance_bin bank_pressure_hour  \n",
       "0  winter        ultra                  1  \n",
       "1  winter        ultra                  9  \n",
       "2  winter        ultra                  1  \n",
       "3  winter        ultra                 11  \n",
       "4  winter        ultra                  1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8b2e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fl_date                   object\n",
       "origin                  category\n",
       "dest                    category\n",
       "crs_dep_time              object\n",
       "dep_delay                float64\n",
       "crs_arr_time              object\n",
       "crs_elapsed_time           int64\n",
       "distance                   int64\n",
       "year                    category\n",
       "route                     object\n",
       "is_us_holiday               bool\n",
       "near_holiday                bool\n",
       "is_summer_peak              bool\n",
       "is_thanksgiving_week        bool\n",
       "is_xmas_newyear             bool\n",
       "is_peak_season              bool\n",
       "weekday                   object\n",
       "is_weekend                  bool\n",
       "dep_hour                category\n",
       "month                   category\n",
       "season                  category\n",
       "distance_bin            category\n",
       "bank_pressure_hour         Int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf5d12c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c7f853d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in PAST (<= 2018-05-31):  785392\n",
      "Rows in FUTURE (>= 2018-06-01): 155495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done! Use 'show' commands to display/save.   |██████████| [100%]   00:00 -> (00:00 left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report output/past_data_report.html was generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done! Use 'show' commands to display/save.   |██████████| [100%]   00:00 -> (00:00 left)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report output/predict_data_report.html was generated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate and save Sweetviz data quality/EDA reports without opening\n",
    "\n",
    "# Parse dates just for splitting (df['fl_date'] can stay object)\n",
    "dt = pd.to_datetime(df[\"fl_date\"], errors=\"coerce\")\n",
    "\n",
    "# Define cutoffs\n",
    "cutoff_past  = pd.Timestamp(\"2018-05-31\")\n",
    "cutoff_predict = pd.Timestamp(\"2018-06-01\")\n",
    "\n",
    "# Create subsets\n",
    "df_past   = df.loc[dt <= cutoff_past].copy()\n",
    "df_predict = df.loc[dt >= cutoff_predict].copy()\n",
    "\n",
    "print(f\"Rows in PAST (<= 2018-05-31):  {len(df_past)}\")\n",
    "print(f\"Rows in FUTURE (>= 2018-06-01): {len(df_predict)}\")\n",
    "\n",
    "sv.analyze(df_past).show_html(\"output/past_data_report.html\", open_browser=False)\n",
    "sv.analyze(df_predict).show_html(\"output/predict_data_report.html\", open_browser=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd81573",
   "metadata": {},
   "source": [
    "Findings in the data quality report:\n",
    "- **SEA is by far the most frequent** origin and destination.\n",
    "- **Mean delay is 1 minute, median is -4**. Average and median **distance is around 1000** (km or mile?).\n",
    "- Years show an increasing frequency, which means that (suppose that our sample is a 100% sample) there are more domestic flights operated by Alaska Airlines every year between 2014-2017 so we can expect an expansion.\n",
    "- I **don't see any obvious signs of data problems**, no duplicates, no missing values (only in delay, but <1%), distribution of variables look ok.\n",
    "- Although, there are **outliers in delay time**, we should think about how to clean it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5970cea5",
   "metadata": {},
   "source": [
    "I also checked in the command line whether append was successful. To calculates the line count in all csv files in the data/raw/ folder, and adds these numbers: \n",
    "```bash\n",
    "sum=0; for f in *.csv; do c=$(wc -l < \"$f\"); echo \"$c  $f\"; sum=$((sum+c)); done; echo \"TOTAL $sum\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6defcaeb",
   "metadata": {},
   "source": [
    "Extend the features with an external source: us the list of US airports to generate a dummy for domestic flights.\n",
    "- origin_domestic is 1 if origin is in the list of airports\n",
    "- dest_domestic is 1 if dest is in the list of airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03cefa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  origin dest  origin_domestic  dest_domestic\n",
      "0    DCA  SEA                1              1\n",
      "1    SEA  DCA                1              1\n",
      "2    DCA  SEA                1              1\n",
      "3    SEA  DCA                1              1\n",
      "4    EWR  SEA                1              1\n"
     ]
    }
   ],
   "source": [
    "# Load domestic airport list\n",
    "airports = pd.read_csv(\"data/raw/airports.csv\")\n",
    "\n",
    "# Normalize column names and build IATA set\n",
    "airports.columns = airports.columns.str.strip().str.upper()\n",
    "iata_set = set(\n",
    "    airports[\"IATA\"].dropna().astype(str).str.strip().str.upper().unique()\n",
    ")\n",
    "\n",
    "# Add dummies to your flights df\n",
    "df[\"origin_domestic\"] = (\n",
    "    df[\"origin\"].astype(str).str.strip().str.upper().isin(iata_set).astype(\"int8\")\n",
    ")\n",
    "df[\"dest_domestic\"] = (\n",
    "    df[\"dest\"].astype(str).str.strip().str.upper().isin(iata_set).astype(\"int8\")\n",
    ")\n",
    "\n",
    "# quick check\n",
    "print(df[[\"origin\",\"dest\",\"origin_domestic\",\"dest_domestic\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c95cbaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    count  mean  std  min  25%  50%  75%  max\n",
      "origin_domestic  940887.0   1.0  0.0  1.0  1.0  1.0  1.0  1.0\n",
      "dest_domestic    940887.0   1.0  0.0  1.0  1.0  1.0  1.0  1.0\n"
     ]
    }
   ],
   "source": [
    "print(df[[\"origin_domestic\",\"dest_domestic\"]].describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5596a6c5",
   "metadata": {},
   "source": [
    "It seems that all flights are domestic, so we can drop this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddbb600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"origin_domestic\", \"dest_domestic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "574d93cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"data/cleaned_df.parquet\")  # fast + preserves dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469bcef8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
